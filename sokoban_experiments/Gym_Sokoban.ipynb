{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gym-Sokoban",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "itEPA5vDZHMh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "f1fed83c-8ccd-49aa-b933-dfdfdbef552b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxahoIC9ZopF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ccc0c383-2006-4c6a-8c7f-1ef4f79f78ca"
      },
      "source": [
        "!ls /gdrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YFdlFByORUnl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d17664be-084d-4e94-c4ff-81c068119c01"
      },
      "source": [
        "# Stable Baselines only supports tensorflow 1.x for now\n",
        "%tensorflow_version 1.x\n",
        "!apt install swig\n",
        "!pip install stable-baselines[mpi]==2.10.0\n",
        "!pip install git+https://github.com/rynsy/gym-sokoban"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-440\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  swig3.0\n",
            "Suggested packages:\n",
            "  swig-doc swig-examples swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  swig swig3.0\n",
            "0 upgraded, 2 newly installed, 0 to remove and 33 not upgraded.\n",
            "Need to get 1,100 kB of archives.\n",
            "After this operation, 5,822 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 1,100 kB in 2s (690 kB/s)\n",
            "Selecting previously unselected package swig3.0.\n",
            "(Reading database ... 144379 files and directories currently installed.)\n",
            "Preparing to unpack .../swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e5/fe/db8159d4d79109c6c8942abe77c7ba6b6e008c32ae55870a35e73fa10db3/stable_baselines-2.10.0-py3-none-any.whl (248kB)\n",
            "\u001b[K     |████████████████████████████████| 256kB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (0.15.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.0.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from stable-baselines[mpi]==2.10.0) (1.18.5)\n",
            "Requirement already satisfied: mpi4py; extra == \"mpi\" in /tensorflow-1.15.2/python3.6 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.0.0)\n",
            "Requirement already satisfied: atari-py~=0.2.0; extra == \"atari\" in /usr/local/lib/python3.6/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2018.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from atari-py~=0.2.0; extra == \"atari\"->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.12.0)\n",
            "Installing collected packages: stable-baselines\n",
            "  Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.0\n",
            "Collecting git+https://github.com/rynsy/gym-sokoban\n",
            "  Cloning https://github.com/rynsy/gym-sokoban to /tmp/pip-req-build-j77su9b3\n",
            "  Running command git clone -q https://github.com/rynsy/gym-sokoban /tmp/pip-req-build-j77su9b3\n",
            "Requirement already satisfied: gym>=0.2.3 in /usr/local/lib/python3.6/dist-packages (from gym-sokoban==0.0.5) (0.17.2)\n",
            "Requirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.6/dist-packages (from gym-sokoban==0.0.5) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.32.1 in /usr/local/lib/python3.6/dist-packages (from gym-sokoban==0.0.5) (4.41.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from gym-sokoban==0.0.5) (2.4.1)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.6/dist-packages (from gym-sokoban==0.0.5) (2.23.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->gym-sokoban==0.0.5) (1.4.1)\n",
            "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->gym-sokoban==0.0.5) (1.3.0)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym>=0.2.3->gym-sokoban==0.0.5) (1.5.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio>=2.3.0->gym-sokoban==0.0.5) (7.0.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->gym-sokoban==0.0.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->gym-sokoban==0.0.5) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->gym-sokoban==0.0.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.22.0->gym-sokoban==0.0.5) (2.9)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym>=0.2.3->gym-sokoban==0.0.5) (0.16.0)\n",
            "Building wheels for collected packages: gym-sokoban\n",
            "  Building wheel for gym-sokoban (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym-sokoban: filename=gym_sokoban-0.0.5-cp36-none-any.whl size=52890 sha256=6966325ee83c7cd9d243977d163d02de0dc3d1c5df1be3349a5dd2a78aa1c778\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-6njpkqpy/wheels/cb/8e/5b/310e63d38e6327d254e04ed1901a4095839e859b3d44931afe\n",
            "Successfully built gym-sokoban\n",
            "Installing collected packages: gym-sokoban\n",
            "Successfully installed gym-sokoban-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grXe85G9RUnp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "a6fc114e-2ab0-442a-ae5b-e14a2be9f8a8"
      },
      "source": [
        "import gym\n",
        "import gym_sokoban\n",
        "import numpy as np\n",
        " \n",
        "from stable_baselines.deepq.policies import LnCnnPolicy \n",
        "from stable_baselines.common.vec_env import DummyVecEnv \n",
        "from stable_baselines import DQN\n",
        "from stable_baselines.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold, CheckpointCallback, CallbackList\n",
        " \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7msZbsYAr_0G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "eaff61d5-803d-4456-db2a-b0c64d3c511e"
      },
      "source": [
        "\"\"\"\n",
        "  Need to save or render environments as you train to see learning rate per level\n",
        "\"\"\"\n",
        "path = F\"/gdrive/My Drive/DQN/\" \n",
        "STEPS_PER_SAVE = 10_000\n",
        " \n",
        "env = gym.make('Sokoban-small-v0')\n",
        "model = DQN(LnCnnPolicy, env, \n",
        "        tensorboard_log=path+\"GRAPH/\", \n",
        "        double_q=True,\n",
        "        prioritized_replay=True,\n",
        "        prioritized_replay_alpha=0.99,\n",
        "        learning_starts=1000,\n",
        "        verbose=1\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/dqn.py:129: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:358: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:359: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:139: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/contrib/layers/python/layers/layers.py:1866: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:147: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:149: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:372: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:415: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/build_graph.py:449: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:241: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py:242: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trgKQpJd91gl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "outputId": "9fea2024-529b-41d3-fabc-9918d36fee45"
      },
      "source": [
        "for i in range(1000):\n",
        "  print(\"=\" * 50, \" ENVIRONMENT #\", i, \" \", \"=\" * 50)\n",
        "  env.reset(regenerate=True)\n",
        "  model.set_env(env)\n",
        "  callback_on_best = StopTrainingOnRewardThreshold(reward_threshold=-9, verbose=1)\n",
        "  eval_callback = EvalCallback(env, callback_on_new_best=callback_on_best, verbose=1)\n",
        "  callback_checkpoint = CheckpointCallback(save_freq=STEPS_PER_SAVE, save_path=path+\"MODEL/\", name_prefix=\"dqn_pr\", verbose=1)\n",
        "  callback_list = CallbackList([eval_callback, callback_checkpoint])\n",
        "  model.learn(total_timesteps=STEPS_PER_SAVE, callback=callback_list)\n",
        "  model.save(path+\"MODEL/final_model_for_env_\"+str(i))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================  ENVIRONMENT # 0   ==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/stable_baselines/common/callbacks.py:277: UserWarning: Training and eval env are not of the same type<SokobanEnv_Small0<Sokoban-small-v0>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6cf1334630>\n",
            "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Eval num_timesteps=10000, episode_reward=-20.00 +/- 0.00\n",
            "Episode length: 200.00 +/- 0.00\n",
            "New best mean reward!\n",
            "==================================================  ENVIRONMENT # 1   ==================================================\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/stable_baselines/common/callbacks.py:277: UserWarning: Training and eval env are not of the same type<SokobanEnv_Small0<Sokoban-small-v0>> != <stable_baselines.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x7f6cf13347f0>\n",
            "  \"{} != {}\".format(self.training_env, self.eval_env))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-c2b5172300bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mcallback_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCheckpointCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_SAVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"MODEL/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_prefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dqn_pr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mcallback_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCallbackList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meval_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback_checkpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mSTEPS_PER_SAVE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"MODEL/final_model_for_env_\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, replay_wrapper)\u001b[0m\n\u001b[1;32m    289\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                             summary, td_errors = self._train_step(obses_t, actions, rewards, obses_tp1, obses_tp1,\n\u001b[0;32m--> 291\u001b[0;31m                                                                   dones, weights, sess=self.sess)\n\u001b[0m\u001b[1;32m    292\u001b[0m                         \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/stable_baselines/common/tf_util.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, sess, *args, **kwargs)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgivens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minpt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs_update\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRBNyQc-7GZr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(env, regen=False):\n",
        "  test_rewards = []\n",
        "  for i in range(10):\n",
        "    test_reward = []\n",
        "    obs = env.reset(regenerate=regen)\n",
        "    while True:\n",
        "      action, _states = model.predict(obs)\n",
        "      obs, rewards, done, info = env.step(action)\n",
        "      test_reward.append(rewards)\n",
        "      if done:\n",
        "        break\n",
        "    test_reward = np.sum(test_reward)\n",
        "    test_rewards.append(test_reward)\n",
        "    print(\"Test {}: {}\".format(i,test_reward))\n",
        "  if regen:\n",
        "    print(\"Average reward for random-board tests: {}\".format(np.average(test_rewards)))\n",
        "  else:\n",
        "    print(\"Average reward for same-board tests: {}\".format(np.average(test_rewards)))\n",
        "  return np.average(test_rewards)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pG7boE-EyBQZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "943616d8-dc02-49af-aed7-14ed2013e69d"
      },
      "source": [
        "STEPS = 10_000\n",
        "env = gym.make('Sokoban-small-v0')\n",
        "model = DQN(LnCnnPolicy, env, \n",
        "     #   tensorboard_log=path+\"GRAPH/\", \n",
        "        double_q=True,\n",
        "        prioritized_replay=True,\n",
        "        prioritized_replay_alpha=0.99,\n",
        "        learning_starts=int(STEPS*0.5),\n",
        "        verbose=2\n",
        "        )\n",
        "\n",
        "for i in range(1000):\n",
        "  print(\"=\" * 50, \" ENVIRONMENT #\", i, \" \", \"=\" * 50)\n",
        "  board_learned = False\n",
        "  while not board_learned:\n",
        "    env.reset(regenerate=board_learned)\n",
        "    model.set_env(env)\n",
        "    model.learn(total_timesteps=STEPS)\n",
        "    reward = test_model(env)\n",
        "    board_learned = reward > 9\n",
        "    if board_learned:\n",
        "      test_model(env, regen=board_learned)\n",
        "      model.save(path+\"MODEL/final_model_for_env_\"+str(i))\n",
        "      print(\"=\" * 50, \" ENVIRONMENT #\", i, \" HAS BEEN LEARNED \", \"=\" * 50)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================================================  ENVIRONMENT # 0   ==================================================\n",
            "Test 0: -19.0\n",
            "Test 1: -19.0\n",
            "Test 2: -19.0\n",
            "Test 3: -19.0\n",
            "Test 4: -19.0\n",
            "Test 5: -19.0\n",
            "Test 6: -19.0\n",
            "Test 7: -19.0\n",
            "Test 8: -19.0\n",
            "Test 9: -19.0\n",
            "Average reward for same-board tests: -19.0\n",
            "Test 0: -19.0\n",
            "Test 1: -19.0\n",
            "Test 2: -19.0\n",
            "Test 3: -19.0\n",
            "Test 4: -19.0\n",
            "Test 5: -19.0\n",
            "Test 6: -19.0\n",
            "Test 7: -19.0\n",
            "Test 8: -19.0\n",
            "Test 9: -19.0\n",
            "Average reward for same-board tests: -19.0\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | -2.1     |\n",
            "| steps                   | 9641     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 2  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 3   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.6      |\n",
            "| steps                   | 1832     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 2962     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 4066     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 10.7     |\n",
            "| steps                   | 5264     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 6357     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.5     |\n",
            "| steps                   | 7740     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.5     |\n",
            "| steps                   | 9156     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 3  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 4   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.9      |\n",
            "| steps                   | 1673     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2489     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3337     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11       |\n",
            "| steps                   | 4308     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.7     |\n",
            "| steps                   | 5500     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11       |\n",
            "| steps                   | 6483     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7328     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 8348     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9168     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 4  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 5   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 8.8      |\n",
            "| steps                   | 2381     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 10.7     |\n",
            "| steps                   | 3529     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4339     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 4        |\n",
            "| steps                   | 9913     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 5  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 6   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.6      |\n",
            "| steps                   | 1774     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2590     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3595     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 4605     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 9.9      |\n",
            "| steps                   | 6347     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7355     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 8364     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 9368     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 6  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 7   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.3      |\n",
            "| steps                   | 1978     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2795     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3609     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4426     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 9.7      |\n",
            "| steps                   | 6220     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 7151     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.6     |\n",
            "| steps                   | 8358     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11       |\n",
            "| steps                   | 9377     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 7  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 8   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.9      |\n",
            "| steps                   | 1606     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2427     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3246     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4066     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4881     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.3     |\n",
            "| steps                   | 6306     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.3     |\n",
            "| steps                   | 7805     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8617     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9427     |\n",
            "--------------------------------------\n",
            "Test 0: -19.0\n",
            "Test 1: -19.0\n",
            "Test 2: -19.0\n",
            "Test 3: -19.0\n",
            "Test 4: -19.0\n",
            "Test 5: -19.0\n",
            "Test 6: -19.0\n",
            "Test 7: -19.0\n",
            "Test 8: -19.0\n",
            "Test 9: -19.0\n",
            "Average reward for same-board tests: -19.0\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 1.3      |\n",
            "| steps                   | 8013     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8834     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9655     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 9  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 10   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.9      |\n",
            "| steps                   | 1666     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2482     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3492     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4305     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 5332     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 6361     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7386     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8204     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9019     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 10  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 11   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10       |\n",
            "| steps                   | 1634     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2459     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3458     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11       |\n",
            "| steps                   | 4426     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.7     |\n",
            "| steps                   | 5632     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6473     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7479     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8299     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9131     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 11  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 12   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.8      |\n",
            "| steps                   | 1713     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2530     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3536     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4355     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 5438     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6252     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7075     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7886     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 8978     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9794     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 12  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 13   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10.1     |\n",
            "| steps                   | 1515     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 10.6     |\n",
            "| steps                   | 2717     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.5     |\n",
            "| steps                   | 4034     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 10.1     |\n",
            "| steps                   | 5572     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.4     |\n",
            "| steps                   | 6896     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7899     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 8827     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 9844     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 13  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 14   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10       |\n",
            "| steps                   | 1589     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2398     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3411     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4234     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.4     |\n",
            "| steps                   | 5619     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6469     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 7321     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8140     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 9148     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9960     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 14  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 15   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10       |\n",
            "| steps                   | 1609     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2423     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3421     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 4431     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 5493     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6311     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7123     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 7988     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8805     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 9916     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 15  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 16   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10.1     |\n",
            "| steps                   | 1558     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2373     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3189     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 4195     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 5227     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6061     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7066     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7887     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 8959     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9802     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 16  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 17   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10       |\n",
            "| steps                   | 1604     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 2607     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3625     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4443     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 11       |\n",
            "| steps                   | 5410     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6223     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7065     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 7930     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 9001     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9823     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 17  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 18   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.7      |\n",
            "| steps                   | 1762     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 2774     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3788     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4604     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 5743     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 6739     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7752     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 8752     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 9760     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 18  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 19   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.6      |\n",
            "| steps                   | 1807     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2625     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3445     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4266     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 5277     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 6193     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7006     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7821     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8641     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9454     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 19  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 20   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10       |\n",
            "| steps                   | 1629     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2441     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 3447     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4260     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.5     |\n",
            "| steps                   | 5542     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 6369     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7190     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 8067     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 10.8     |\n",
            "| steps                   | 9155     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 20  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 21   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 10.1     |\n",
            "| steps                   | 1573     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2388     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3211     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4031     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 4847     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 11.1     |\n",
            "| steps                   | 5700     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 6707     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 7519     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 900      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8335     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 1000     |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 9148     |\n",
            "--------------------------------------\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for same-board tests: 11.200000000000001\n",
            "Test 0: 11.200000000000001\n",
            "Test 1: 11.200000000000001\n",
            "Test 2: 11.200000000000001\n",
            "Test 3: 11.200000000000001\n",
            "Test 4: 11.200000000000001\n",
            "Test 5: 11.200000000000001\n",
            "Test 6: 11.200000000000001\n",
            "Test 7: 11.200000000000001\n",
            "Test 8: 11.200000000000001\n",
            "Test 9: 11.200000000000001\n",
            "Average reward for random-board tests: 11.200000000000001\n",
            "==================================================  ENVIRONMENT # 21  HAS BEEN LEARNED  ==================================================\n",
            "==================================================  ENVIRONMENT # 22   ==================================================\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 100      |\n",
            "| mean 100 episode reward | 9.3      |\n",
            "| steps                   | 2005     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 200      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 2832     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 300      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 3653     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 400      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 4661     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 500      |\n",
            "| mean 100 episode reward | 10.7     |\n",
            "| steps                   | 5816     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 600      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 6817     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 700      |\n",
            "| mean 100 episode reward | 10.9     |\n",
            "| steps                   | 7842     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 800      |\n",
            "| mean 100 episode reward | 11.2     |\n",
            "| steps                   | 8650     |\n",
            "--------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}